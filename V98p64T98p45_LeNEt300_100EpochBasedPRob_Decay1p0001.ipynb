{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V98p64T98p45_LeNEt300_100EpochBasedPRob_Decay1p0001.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rTq2SzhyZgD",
        "colab_type": "code",
        "outputId": "6a0a63c8-7e37-4a16-e06a-c10d1777528f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "assert(len(X_train) == len(y_train))\n",
        "assert(len(X_validation) == len(y_validation))\n",
        "assert(len(X_test) == len(y_test))\n",
        "\n",
        "print()\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print()\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-566519a95339>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "\n",
            "Image Shape: (28, 28, 1)\n",
            "\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lopHk729ydu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = X_train.reshape(X_train.shape[0],-1)\n",
        "train_label = y_train\n",
        "validation_data = X_validation.reshape(X_validation.shape[0],-1)\n",
        "validation_label = y_validation\n",
        "test_data = X_test.reshape(X_test.shape[0],-1)\n",
        "test_label = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "3beb9c43-677f-411c-ca4a-226f2be37aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "# clf.fit(train_valid_combined, train_valid_label)\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.27088284\n",
            "Iteration 2, loss = 0.09284897\n",
            "Iteration 3, loss = 0.06362837\n",
            "Iteration 4, loss = 0.04513577\n",
            "Iteration 5, loss = 0.03265040\n",
            "Iteration 6, loss = 0.02556718\n",
            "Iteration 7, loss = 0.01983507\n",
            "Iteration 8, loss = 0.01265771\n",
            "Iteration 9, loss = 0.00954815\n",
            "Iteration 10, loss = 0.00746641\n",
            "Iteration 11, loss = 0.00487287\n",
            "Iteration 12, loss = 0.00280570\n",
            "Iteration 13, loss = 0.00162220\n",
            "Iteration 14, loss = 0.00092841\n",
            "Iteration 15, loss = 0.00073810\n",
            "Iteration 16, loss = 0.00063660\n",
            "Iteration 17, loss = 0.00065059\n",
            "Iteration 18, loss = 0.00056893\n",
            "Iteration 19, loss = 0.00052885\n",
            "Iteration 20, loss = 0.00050482\n",
            "Iteration 21, loss = 0.00049094\n",
            "Iteration 22, loss = 0.00047780\n",
            "Iteration 23, loss = 0.00046712\n",
            "Iteration 24, loss = 0.00045812\n",
            "Iteration 25, loss = 0.00044891\n",
            "Iteration 26, loss = 0.00044162\n",
            "Iteration 27, loss = 0.00043538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "d80b3a84-ad14-4120-d8ba-1b848f4f7e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.25907034\n",
            "Iteration 2, loss = 0.09239542\n",
            "Iteration 3, loss = 0.06153412\n",
            "Iteration 4, loss = 0.04633119\n",
            "Iteration 5, loss = 0.03438245\n",
            "Iteration 6, loss = 0.02314845\n",
            "Iteration 7, loss = 0.01970873\n",
            "Iteration 8, loss = 0.01600088\n",
            "Iteration 9, loss = 0.01225262\n",
            "Iteration 10, loss = 0.01059671\n",
            "Iteration 11, loss = 0.00743295\n",
            "Iteration 12, loss = 0.00342327\n",
            "Iteration 13, loss = 0.00159669\n",
            "Iteration 14, loss = 0.00100458\n",
            "Iteration 15, loss = 0.00071399\n",
            "Iteration 16, loss = 0.00058734\n",
            "Iteration 17, loss = 0.00055026\n",
            "Iteration 18, loss = 0.00052253\n",
            "Iteration 19, loss = 0.00050488\n",
            "Iteration 20, loss = 0.00048858\n",
            "Iteration 21, loss = 0.00047578\n",
            "Iteration 22, loss = 0.00046572\n",
            "Iteration 23, loss = 0.00045739\n",
            "Iteration 24, loss = 0.00044846\n",
            "Iteration 25, loss = 0.00044014\n",
            "Iteration 26, loss = 0.00043509\n",
            "Iteration 27, loss = 0.00042904\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "dbe4a5aa-c1d0-45ef-8b98-90a2c0a3793c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "6e154418-e7c8-42e6-cded-d20cb89a6537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "b2f672cb-53d6-4cbf-a96d-a7aed6184f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "d8792245-7bec-4c3a-8d7b-9fe43558ec8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "9f7c75c3-d46d-4f0e-f32f-1d9260f15319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "fab73f6f-e49e-4463-879a-3e9faee17458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "e735042f-a2b7-4f65-9818-00014a96c876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "HFVqYNiRCy47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LeNet Lab\n",
        "![LeNet Architecture](https://github.com/sujaybabruwad/LeNet-in-Tensorflow/blob/master/lenet.png?raw=1)\n",
        "Source: Yan LeCun"
      ]
    },
    {
      "metadata": {
        "id": "oWv2YnX4IxqB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.tensorboard.plugins import projector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7PXv7fjCy5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "\n",
        "Load the MNIST data, which comes pre-loaded with TensorFlow.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "t4iqynjrCy5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "# X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "# X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "# X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "# assert(len(X_train) == len(y_train))\n",
        "# assert(len(X_validation) == len(y_validation))\n",
        "# assert(len(X_test) == len(y_test))\n",
        "\n",
        "# print()\n",
        "# print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "# print()\n",
        "# print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "# print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "# print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LwdNZ5guEpy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Jm9y17KI5QD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "logs_path = \"./logs/embedding/\"  # path to the folder that we want to save the logs for Tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pV8kvNmMCy5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
        "\n",
        "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
        "\n",
        "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QBcHY4jyzH5J",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Pad images with 0s\n",
        "# X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "# X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "# X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    \n",
        "# print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6o1m5ujwMvb",
        "colab_type": "code",
        "outputId": "5528195a-a35f-4248-a1e8-84571e03f370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "fsvvfG8bwGOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train_reshaped = np.reshape(X_train, newshape=(X_train.shape[0],-1))\n",
        "# X_validation_reshaped= np.reshape(X_validation, newshape=(X_validation.shape[0],-1))\n",
        "# X_test_reshaped= np.reshape(X_test, newshape=(X_test.shape[0],-1))\n",
        "\n",
        "# # X_train_reshaped.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eaMcBCNCvzWq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "# #                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "# #                     learning_rate_init=.1)\n",
        "# ### acc is 98.41\n",
        "# clf = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "# # Test set score: 0.950119\n",
        "\n",
        "# # clf.fit(train_valid_combined, train_valid_label)\n",
        "# clf.fit(X_train_reshaped, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzbhg-ryyhkD",
        "colab_type": "code",
        "outputId": "a46fe54e-8d0e-4f55-8070-374b345f69b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5444., 6179., 5470., 5638., 5307., 4987., 5417., 5715., 5389.,\n",
              "        5454.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sfj08N7cwqqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf.score(X_train_reshaped,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTE-oU9NwssQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf.score(X_validation_reshaped,y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBteDY2jwuF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf.score(X_test_reshaped,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cW7cCH1xCy5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize Data\n",
        "\n",
        "View a sample from the dataset.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "h7UbBX8RCy5e",
        "colab_type": "code",
        "outputId": "6c94852b-88ed-4e83-96c7-b7098778a54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "index = random.randint(0, len(X_train))\n",
        "image = X_train[index].squeeze()\n",
        "\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "print(y_train[index])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBvkfyw+o37H7ys8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1W6py5mlCy5n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data\n",
        "\n",
        "Shuffle the training data.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cWVHSK0pCy59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TODO: Implement LeNet-5\n",
        "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
        "\n",
        "This is the only cell you need to edit.\n",
        "### Input\n",
        "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
        "\n",
        "### Architecture\n",
        "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Pooling.** The output shape should be 14x14x6.\n",
        "\n",
        "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Pooling.** The output shape should be 5x5x16.\n",
        "\n",
        "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
        "\n",
        "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
        "\n",
        "### Output\n",
        "Return the result of the 2nd fully connected layer."
      ]
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "outputId": "32718a18-9322-4ebf-b72c-eee7dd08ac1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(.9999)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqIQlX7Rtqrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Se2Hp4KQvCW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iov1Mky-vohQ",
        "colab_type": "code",
        "outputId": "6fb0f9ed-96a3-4bb1-8857-2aa67d298fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[2].shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "X1nJmcI7Cy5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "    fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "    # TODO: Activation.\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(tf.truncated_normal(shape = [X_train.shape[1]*X_train.shape[2],100], mean = mu, stddev = sigma))\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1]*X_train.shape[2],clf.coefs_[1].shape[1]]))\n",
        "\n",
        "#     fc2p_b = tf.Variable(tf.zeros(clf.coefs_[1].shape[1]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "    fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    fc2 = tf.cond(connect2,lambda: fc2 + fc2_2nd_input, lambda: fc2 )    \n",
        "    ################\n",
        "    \n",
        "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
        "#     fc3_w = tf.Variable(tf.truncated_normal(shape = (clf.coefs_[1].shape[1],10), mean = mu , stddev = sigma))\n",
        "#     fc3_w = tf.Variable(tf.truncated_normal(shape = (clf.coefs_[1].shape[1],10), mean = mu , stddev = sigma))\n",
        "\n",
        "#     fc3_b = tf.Variable(tf.zeros(10))\n",
        "    fc3_w = G_W3\n",
        "    fc3_b = G_b3\n",
        "    \n",
        "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2V3XEMaYvjDW",
        "colab_type": "code",
        "outputId": "acdcb03d-f85b-441b-d181-072a98b78da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "aCx837c9Cy6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Features and Labels\n",
        "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
        "\n",
        "`x` is a placeholder for a batch of input images.\n",
        "`y` is a placeholder for a batch of output labels.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "iafVK0DgLRTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mrb2FHFHCy6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, 28, 28, 1), name='X')\n",
        "  tf.summary.image('input_image', x, max_outputs=5)\n",
        "  y = tf.placeholder(tf.int32, (None), name = 'Y')\n",
        "one_hot_y = tf.one_hot(y, 10)\n",
        "is_testing= tf.placeholder(tf.bool) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uk0pcAjmCy6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Pipeline\n",
        "Create a training pipeline that uses the model to classify MNIST data.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "BPo6q2KkpDVA",
        "colab_type": "code",
        "outputId": "8e59c7ef-ae39-40f7-be54-df8cc7c0f4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape[0]/BATCH_SIZE"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107.421875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "dvZ1AxLnoyr2",
        "colab_type": "code",
        "outputId": "f743143a-1da3-4553-fc53-0c93111588d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "(55000/512)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107.421875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "37lsIW7Zo7bB",
        "colab_type": "code",
        "outputId": "9e01a2e6-20c7-48c3-ff0e-01abc99d2641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "1.0002**107.421875"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0217146310875027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "3oYy_Y3FCy6N",
        "colab_type": "code",
        "outputId": "8cfea5db-615a-4c4e-adcd-a3def69b696c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0001**(X_train.shape[0]/BATCH_SIZE);\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "  loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0107995490766877\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-38-821d9a302873>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRw7tBqHvnh9",
        "colab_type": "code",
        "outputId": "173bb8ee-e424-45ab-cb14-432296a24b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "(1/decay_rate)**200"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11667883888773667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "KrMtHo6rCy6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "OMR9XsLBCy6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    tot_acc = total_accuracy / num_examples\n",
        "    with tf.name_scope('Accuracy'):\n",
        "      tf.summary.scalar('accuracy', tot_acc)\n",
        "    return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Jw7iox3Cy6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "Run the training data through the training pipeline to train the model.\n",
        "\n",
        "Before each epoch, shuffle the training set.\n",
        "\n",
        "After each epoch, measure the loss and accuracy of the validation set.\n",
        "\n",
        "Save the model after training.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "c98G07hDQG2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8QC1kt_DL1yU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def augment(images, labels,\n",
        "            resize=None, # (width, height) tuple or None\n",
        "            horizontal_flip=False,\n",
        "            vertical_flip=False,\n",
        "            rotate=0, # Maximum rotation angle in degrees\n",
        "            crop_probability=0, # How often we do crops\n",
        "            crop_min_percent=0.6, # Minimum linear dimension of a crop\n",
        "            crop_max_percent=1.,  # Maximum linear dimension of a crop\n",
        "            mixup=0):  # Mixup coeffecient, see https://arxiv.org/abs/1710.09412.pdf\n",
        "  if resize is not None:\n",
        "    images = tf.image.resize_bilinear(images, resize)\n",
        "  \n",
        "  # My experiments showed that casting on GPU improves training performance\n",
        "  if images.dtype != tf.float32:\n",
        "    images = tf.image.convert_image_dtype(images, dtype=tf.float32)\n",
        "    images = tf.subtract(images, 0.5)\n",
        "    images = tf.multiply(images, 2.0)\n",
        "  labels = tf.to_float(labels)\n",
        "\n",
        "  with tf.name_scope('augmentation'):\n",
        "    shp = tf.shape(images)\n",
        "    batch_size, height, width = shp[0], shp[1], shp[2]\n",
        "    width = tf.cast(width, tf.float32)\n",
        "    height = tf.cast(height, tf.float32)\n",
        "\n",
        "    # The list of affine transformations that our image will go under.\n",
        "    # Every element is Nx8 tensor, where N is a batch size.\n",
        "    transforms = []\n",
        "    identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
        "    if horizontal_flip:\n",
        "      coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
        "      flip_transform = tf.convert_to_tensor(\n",
        "          [-1., 0., width, 0., 1., 0., 0., 0.], dtype=tf.float32)\n",
        "      transforms.append(\n",
        "          tf.where(coin,\n",
        "                   tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
        "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
        "\n",
        "    if vertical_flip:\n",
        "      coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
        "      flip_transform = tf.convert_to_tensor(\n",
        "          [1, 0, 0, 0, -1, height, 0, 0], dtype=tf.float32)\n",
        "      transforms.append(\n",
        "          tf.where(coin,\n",
        "                   tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
        "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
        "\n",
        "    if rotate > 0:\n",
        "      angle_rad = rotate / 180 * math.pi\n",
        "      angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n",
        "      transforms.append(\n",
        "          tf.contrib.image.angles_to_projective_transforms(\n",
        "              angles, height, width))\n",
        "\n",
        "    if crop_probability > 0:\n",
        "      crop_pct = tf.random_uniform([batch_size], crop_min_percent,\n",
        "                                   crop_max_percent)\n",
        "      left = tf.random_uniform([batch_size], 0, width * (1 - crop_pct))\n",
        "      top = tf.random_uniform([batch_size], 0, height * (1 - crop_pct))\n",
        "      crop_transform = tf.stack([\n",
        "          crop_pct,\n",
        "          tf.zeros([batch_size]), top,\n",
        "          tf.zeros([batch_size]), crop_pct, left,\n",
        "          tf.zeros([batch_size]),\n",
        "          tf.zeros([batch_size])\n",
        "      ], 1)\n",
        "\n",
        "      coin = tf.less(\n",
        "          tf.random_uniform([batch_size], 0, 1.0), crop_probability)\n",
        "      transforms.append(\n",
        "          tf.where(coin, crop_transform,\n",
        "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
        "\n",
        "    if transforms:\n",
        "      images = tf.contrib.image.transform(\n",
        "          images,\n",
        "          tf.contrib.image.compose_transforms(*transforms),\n",
        "          interpolation='BILINEAR') # or 'NEAREST'\n",
        "\n",
        "    def cshift(values): # Circular shift in batch dimension\n",
        "      return tf.concat([values[-1:, ...], values[:-1, ...]], 0)\n",
        "\n",
        "    if mixup > 0:\n",
        "      mixup = 1.0 * mixup # Convert to float, as tf.distributions.Beta requires floats.\n",
        "      beta = tf.distributions.Beta(mixup, mixup)\n",
        "      lam = beta.sample(batch_size)\n",
        "      ll = tf.expand_dims(tf.expand_dims(tf.expand_dims(lam, -1), -1), -1)\n",
        "      images = ll * images + (1 - ll) * cshift(images)\n",
        "      labels = lam * labels + (1 - lam) * cshift(labels)\n",
        "\n",
        "  return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siI7jT2RUvYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def augment_data(dataset, dataset_labels, augementation_factor=1, use_random_rotation=True, use_random_shear=True, use_random_shift=True, use_random_zoom=True):\n",
        "\taugmented_image = []\n",
        "\taugmented_image_labels = []\n",
        "\n",
        "\tfor num in range (0, dataset.shape[0]):\n",
        "\n",
        "\t\tfor i in range(0, augementation_factor):\n",
        "\t\t\t# original image:\n",
        "\t\t\taugmented_image.append(dataset[num])\n",
        "\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "\t\t\tif use_random_rotation:\n",
        "\t\t\t\taugmented_image.append(tf.contrib.keras.preprocessing.image.random_rotation(dataset[num], 20, row_axis=0, col_axis=1, channel_axis=2))\n",
        "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "\t\t\tif use_random_shear:\n",
        "\t\t\t\taugmented_image.append(tf.contrib.keras.preprocessing.image.random_shear(dataset[num], 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
        "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "\t\t\tif use_random_shift:\n",
        "\t\t\t\taugmented_image.append(tf.contrib.keras.preprocessing.image.random_shift(dataset[num], 0.2, 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
        "\t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "# \t\t\tif use_random_zoom:\n",
        "# \t\t\t\taugmented_image.append(tf.contrib.keras.preprocessing.image.random_zoom(dataset[num], 0.9, row_axis=0, col_axis=1, channel_axis=2))\n",
        "# \t\t\t\taugmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "\treturn np.array(augmented_image), np.array(augmented_image_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Of2PvjkIQikg",
        "colab_type": "code",
        "outputId": "d35b20c5-0205-4f62-b598-1c7f85c32ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "(X_train.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "Eb8X_yO6QDIx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sw7vjTrPxBDU",
        "colab_type": "code",
        "outputId": "e3642f64-98bc-4fec-a3e0-6a65c3b62a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "MbueMHwfEbi4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "BATCH_SIZE= 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INEVMxw4R0Nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mnist.test.images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYoZ96svRxfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the test set\n",
        "x_test = mnist.test.images\n",
        "y_test = mnist.test.labels\n",
        "\n",
        "# Initialize the embedding variable with the shape of our desired tensor\n",
        "tensor_shape = (x_test.shape[0] , logits.get_shape()[1].value) # [test_set , h1] = [10000 , 200]\n",
        "embedding_var = tf.Variable(tf.zeros(tensor_shape), \n",
        "                            name='logits_embedding')\n",
        "# assign the tensor that we want to visualize to the embedding variable\n",
        "embedding_assign = embedding_var.assign(logits) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8QrvPzlGSN6S",
        "colab_type": "code",
        "outputId": "b6a4e6bf-1146-434e-b88c-2a775161e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# EPOCHS = 2\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "t26ZbqMVoNRr",
        "colab_type": "code",
        "outputId": "ea357c1b-9125-473c-b11c-7238a1e36fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "bKU1MIyJCy6Y",
        "colab_type": "code",
        "outputId": "afdbfd3c-294e-4320-e15b-929761720d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "cell_type": "code",
      "source": [
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
        "\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:,:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:,:,:],y_train[step:finish]\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "  \n",
        "  \n",
        "#         for offset in range(0, num_examples + BATCH_SIZE, BATCH_SIZE):\n",
        "#             end = offset + BATCH_SIZE\n",
        "#             if end>X_train.shape[0]:\n",
        "#               end = X_train.shape[0]\n",
        "#             print(offset,end)\n",
        "#             batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "            tr_op,summary_tr  = sess.run([training_operation,merged], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "            train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "#         print(prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './lenet1p0001')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 0.98540\n",
            "0.9892169\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 0.98520\n",
            "0.88846695\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 0.98540\n",
            "0.7979782\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 0.98540\n",
            "0.71670556\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 0.98580\n",
            "0.6437104\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 0.98580\n",
            "0.5781497\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 0.98540\n",
            "0.5192661\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.4663798\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.4188799\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.37621775\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.33790064\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.30348608\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 0.98600\n",
            "0.27257657\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 0.98540\n",
            "0.24481511\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.21988115\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 0.98580\n",
            "0.19748667\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 0.98560\n",
            "0.177373\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 0.98600\n",
            "0.15930788\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 0.98640\n",
            "0.14308266\n",
            "\n",
            "Train Accuracy = 1.00000\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 0.98600\n",
            "0.12850994\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QOmTLbuCIaZJ",
        "colab_type": "code",
        "outputId": "edee3593-d89e-454f-9b74-bf394feb07a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9863999964714051"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "hUc5BqhdSVtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_w = 28\n",
        "img_h = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tr1bfaPASS3n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "# Create a config object to write the configuration parameters\n",
        "config = projector.ProjectorConfig()\n",
        "\n",
        "# Add embedding variable\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = embedding_var.name\n",
        "\n",
        "# Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "\n",
        "# Specify where you find the sprite. -> we will create this image later\n",
        "embedding.sprite.image_path = 'sprite_images.png'\n",
        "embedding.sprite.single_image_dim.extend([img_w, img_h])\n",
        "\n",
        "# Write a projector_config.pbtxt in the logs_path.\n",
        "# TensorBoard will read this file during startup.\n",
        "projector.visualize_embeddings(train_writer, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JnCs7sUUcLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Run session to evaluate the tensor\n",
        "# with tf.Session() as sess:\n",
        "#   x_test_fc1 = sess.run(embedding_assign, feed_dict={x: X_test})\n",
        "\n",
        "#   # Save the tensor in model.ckpt file\n",
        "#   saver = tf.train.Saver()\n",
        "#   saver.save(sess, os.path.join(logs_path, \"model.ckpt\"), i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVIeYA_cCy6e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model\n",
        "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
        "\n",
        "Be sure to only do this once!\n",
        "\n",
        "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "O4b5lSD5H_q9",
        "colab_type": "code",
        "outputId": "df050988-1b55-4a21-ce77-3d68265ed3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './lenet1p0001')\n",
        "    test_accuracy = evaluate(X_validation, y_validation)\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./lenet1p0001\n",
            "Validation Accuracy = 0.986400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-KeugG2UcKb",
        "colab_type": "code",
        "outputId": "ffe1862d-9149-43c6-df9e-3c4d64e81e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(validation_accuracy_track)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdf8c7c76a0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4wZkwUKY4wxY7JAYYwxZkz/H6JtIC/7xX+oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "r8bS_3E0qKH1",
        "colab_type": "code",
        "outputId": "3e710f37-74fe-4a1b-a353-18dedfdc06f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './lenet1p0001')\n",
        "    test_accuracy = evaluate(X_test, y_test)\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./lenet1p0001\n",
            "Test Accuracy = 0.984500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V7Uw7Yj0UaaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIMkpCgsTwRv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_tmzQ7mUs8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = [k for k in range(1,201,10)]\n",
        "# epochTrack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AOxPCQoOUSjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('LeNEt300-100Tracked.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epochTrack})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ldZP-eobLvM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sio.savemat('LeNEt300-100TrackedV2Test98p42Valid98p74tNoDropConnection.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "#                                        'train_accuracy_track':train_accuracy_track,\n",
        "#                                        'connection_probability_track':connection_probability_track,\n",
        "#                                        'epochTrack':epochTrack, 'TestAcc':test_accuracy,\n",
        "#                                                          'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZQ52QIrztzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('V98p64T98p45_LeNEt300_100EpochBasedPRob_Decaylenet1p0001.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epochTrack, 'TestAcc':test_accuracy,\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVXJD4prT0sh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "{'ValidationTracked'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gATWCb50-pbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './lenet')\n",
        "#     test_accuracy = evaluate(X_test, y_test)\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJONJ7CeCy6f",
        "colab_type": "code",
        "outputId": "99ef394d-dcd7-4efc-d863-1bb1c8dfd18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3131
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './lenet')\n",
        "    x_test_fc1 = sess.run(embedding_assign, feed_dict={x: X_test,is_testing: True})\n",
        "\n",
        "    # Save the tensor in model.ckpt file\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, os.path.join(logs_path, \"model.ckpt\"), i)\n",
        "    test_accuracy = evaluate(X_test, y_test)\n",
        "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./lenet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Key logits_embedding_3 not found in checkpoint\n\t [[{{node save_5/RestoreV2}}]]\n\t [[{{node save_5/RestoreV2}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Key logits_embedding_3 not found in checkpoint\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\nCaused by op 'save_5/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-76-789083789e7d>\", line 7, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key logits_embedding_3 not found in checkpoint\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   object_graph_string = reader.get_tensor(\n\u001b[0;32m-> 1591\u001b[0;31m       checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[1;32m   1592\u001b[0m   object_graph_proto = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    369\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 370\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-2a49aef686ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     saver.restore(sess, './lenet')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_test_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_assign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1292\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey logits_embedding_3 not found in checkpoint\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\nCaused by op 'save_5/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-76-789083789e7d>\", line 7, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey logits_embedding_3 not found in checkpoint\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n\t [[node save_5/RestoreV2 (defined at <ipython-input-76-789083789e7d>:7) ]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qvRqnUrrCy6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_sprite_image(filename, images):\n",
        "    \"\"\"\n",
        "        Create a sprite image consisting of sample images\n",
        "        :param filename: name of the file to save on disk\n",
        "        :param shape: tensor of flattened images\n",
        "    \"\"\"\n",
        "\n",
        "    # Invert grayscale image\n",
        "    images = 1 - images\n",
        "\n",
        "    # Calculate number of plot\n",
        "    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n",
        "\n",
        "    # Make the background of sprite image\n",
        "    sprite_image = np.ones((img_h * n_plots, img_w * n_plots))\n",
        "\n",
        "    for i in range(n_plots):\n",
        "        for j in range(n_plots):\n",
        "            img_idx = i * n_plots + j\n",
        "            if img_idx < images.shape[0]:\n",
        "                img = images[img_idx]\n",
        "                sprite_image[i * img_h:(i + 1) * img_h,\n",
        "                j * img_w:(j + 1) * img_w] = img\n",
        "\n",
        "    plt.imsave(filename, sprite_image, cmap='gray')\n",
        "    print('Sprite image saved in {}'.format(filename))\n",
        "\n",
        "def write_metadata(filename, labels):\n",
        "    \"\"\"\n",
        "            Create a metadata file image consisting of sample indices and labels\n",
        "            :param filename: name of the file to save on disk\n",
        "            :param shape: tensor of labels\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"Index\\tLabel\\n\")\n",
        "        for index, label in enumerate(labels):\n",
        "            f.write(\"{}\\t{}\\n\".format(index, label))\n",
        "\n",
        "    print('Metadata file saved in {}'.format(filename))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vE0ULEJ7XGlH",
        "colab_type": "code",
        "outputId": "521734b2-6287-4ee4-e907-495d334f9f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "F-zHFsYHWvrv",
        "colab_type": "code",
        "outputId": "54ffa52c-c52a-47e8-a521-bb74797f586b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Reshape images from vector to matrix\n",
        "x_test_images = np.reshape(np.array(X_test), (-1, img_w, img_h))\n",
        "# Reshape labels from one-hot-encode to index\n",
        "x_test_labels = y_test\n",
        "\n",
        "write_sprite_image(os.path.join(logs_path, 'sprite_images.png'), x_test_images)\n",
        "write_metadata(os.path.join(logs_path, 'metadata.tsv'), x_test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sprite image saved in ./logs/embedding/sprite_images.png\n",
            "Metadata file saved in ./logs/embedding/metadata.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uPVpeenpW8sp",
        "colab_type": "code",
        "outputId": "23e1a68e-78ab-4b89-d0ff-571473bf15b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip\n",
        "# logdir=logs/embedding/\n",
        "LOG_DIR = 'logs/embedding/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-07 22:31:50--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.73.94.166, 52.45.111.123, 52.4.95.48, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.73.94.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ngrok-stable-linux-amd64.zip\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  7.04MB/s    in 2.0s    \n",
            "\n",
            "2019-04-07 22:31:53 (7.04 MB/s) - ngrok-stable-linux-amd64.zip saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sULv8PgRXp2j",
        "colab_type": "code",
        "outputId": "7e8c41b2-028f-4d9d-ff5b-4cf9db24e07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ngrok_url = !curl -s http://localhost:4040/api/tunnels | python -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "        \n",
        "ngrok_url = ngrok_url[0].replace(\"'\", '')\n",
        "print(ngrok_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://d310ebff.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q7TjVgm-X6N1",
        "colab_type": "code",
        "outputId": "b58870c5-0ee1-4f23-f15e-fd4e0a933754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(ngrok_url, width=700, height=900)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"700\"\n",
              "            height=\"900\"\n",
              "            src=\"https://d310ebff.ngrok.io\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f82b37fed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "u5OZDSZ7X9eO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}